{"cells":[{"cell_type":"code","source":["dbutils.fs.put(\"/scenarios/Sample_NULL.csv\",\"\"\"id,name,loc,updated_date,Purpose\n1,ravi,bangalore,2021-01-01,\n1,ravi,chennai,2022-02-02,\n1,ravi,Hyderabad,2022-06-10,\n2,Raj,bangalore,2021-01-01,\n2,Raj,chennai,2022-02-02,\n3,Raj,Hyderabad,2022-06-10,\n4,Prasad,bangalore,2021-01-01,\n5,Mahesh,chennai,2022-02-02,\n4,Prasad,Hyderabad,2022-06-10,\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1d142df8-796e-4170-8bf1-e993671869dc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Wrote 291 bytes.\nOut[1]: True"]}],"execution_count":0},{"cell_type":"code","source":["df1=spark.read.csv(\"/scenarios/Sample_NULL.csv\",header=True,inferSchema=True)\ndf1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"24c94784-9fb2-4ac0-be31-015c4ecf0bed","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------+---------+------------+-------+\n| id|  name|      loc|updated_date|Purpose|\n+---+------+---------+------------+-------+\n|  1|  ravi|bangalore|  2021-01-01|   null|\n|  1|  ravi|  chennai|  2022-02-02|   null|\n|  1|  ravi|Hyderabad|  2022-06-10|   null|\n|  2|   Raj|bangalore|  2021-01-01|   null|\n|  2|   Raj|  chennai|  2022-02-02|   null|\n|  3|   Raj|Hyderabad|  2022-06-10|   null|\n|  4|Prasad|bangalore|  2021-01-01|   null|\n|  5|Mahesh|  chennai|  2022-02-02|   null|\n|  4|Prasad|Hyderabad|  2022-06-10|   null|\n+---+------+---------+------------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["## Check the count of each columns in DF\nfrom pyspark.sql import functions as F\ndf2= df1.agg(*[F.count(c).alias(c) for c in df1.columns])\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b0305539-9814-4d74-b93a-c5a9a833d00b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+---+------------+-------+\n| id|name|loc|updated_date|Purpose|\n+---+----+---+------------+-------+\n|  9|   9|  9|           9|      0|\n+---+----+---+------------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["## If the count of columns is zero,it is having Null Value\nNonNull_columns=[c for c in df2.columns if df2[[c]].first()[c]>0]\ntype(NonNull_columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f3575545-9474-4480-9ecd-e91d4c4c23b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[13]: list"]}],"execution_count":0},{"cell_type":"code","source":["## Select the columns which is having NOt Null values\ndf_final=df2.select(*NonNull_columns)\ndf_final.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2ad2c77d-bf7d-40c6-b754-4b3bcc6edc6f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+---+------------+\n| id|name|loc|updated_date|\n+---+----+---+------------+\n|  9|   9|  9|           9|\n+---+----+---+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40cee724-57f2-4432-9121-118b7b7fc04b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"13.How to drop columns which is having All NULL values","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4428315185304929}},"nbformat":4,"nbformat_minor":0}
